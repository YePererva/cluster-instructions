# 1. Assigning the static IP addresses to each of computing nodes

## Operation on or from managing node

### Identify MAC-addresses of all attached nodes
Read MAC addresses of all computers attached to a certain interface (`eno1`):
```
sudo arp-scan --interface=eno2 --localnet
```
It will return all devices attached to the `eno2` interface according to subnet mask specified in `/etc/sysconfig/network-scripts/ifcfg-eno2`.

The response should looks like:
```
Interface: eno2, type: EN10MB, MAC: 08:9e:01:a0:63:86, IPv4: 10.42.42.1
Starting arp-scan 1.9.7 with 256 hosts (https://github.com/royhills/arp-scan)
10.42.42.2      00:23:7d:5e:e1:9c       Dell
10.42.42.40     00:23:7d:5e:e1:9a       Hewlett Packard
10.42.42.209    00:23:7d:5e:e0:b6       Hewlett Packard
10.42.42.238    00:23:7d:5e:e1:90       Hewlett Packard
10.42.42.239    00:23:7d:5e:cf:2c       Hewlett Packard
```
Where `10.42.42.2` is a shared storage address. We need list of MAC addresses of all nodes. They are:
```
00:23:7d:5e:e1:9a
00:23:7d:5e:e0:b6
00:23:7d:5e:e1:90
00:23:7d:5e:cf:2c
```
For each found MAC address we need to assign an individual static IP address. To do that, edit `/etc/dhcp/dhcpd.conf` and modify the network setup.
For each node we need to add fragment based on template:
```
host node_X {
    hardware ethernet `MAC_ADDRESS_of_node_X`;
    fixed-address 10.42.42.X;
}
```

As example:
```
host node_1 {
    hardware ethernet 00:23:7d:5e:e1:9a;
    fixed-address 10.42.42.10;
}
host node_2 {
    hardware ethernet 00:23:7d:5e:e0:b6;
    fixed-address 10.42.42.11;
}
host node_3 {
    hardware ethernet 00:23:7d:5e:e1:90;
    fixed-address 10.42.42.12;
}
host node_4 {
    hardware ethernet 00:23:7d:5e:cf:2c;
    fixed-address 10.42.42.13;
}
```

Now restart `dhcp` server and check if it worked:
```
sudo systemctl restart dhcpd && wait
sudo arp-scan --interface=eno2 --localnet
```
Sometimes, reboot may be required (of computation nodes, not managing nodes), because re-setting `dhcpd` may take a while.

### Adjust `hosts` file for discovery of nodes by each other
SLURM suggests to name nodes in manner of `{some name}{number}` (without curvy brackets), where:
- `some name`: can be any word or string
- `number`: any integer

In a manner of speaking, cluster is a city or street, where each node is a separate house with number. So, for a sake of example, lets use `kyiv` as a name. Now on managing node open `/etc/hosts` file, where is something like:
```
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
```
We need to add each node in cluster according to template:
```
{node_x_IP} {node_x_hostname}
```
In our case, something like:
```
10.42.42.1  kyiv0
10.42.42.2  kyiv_storage
10.42.42.10 kyiv1
10.42.42.11 kyiv2
10.42.42.12 kyiv3
10.42.42.13 kyiv4
```
Do it for `/etc/hosts` on each node!

Now,  check the connectivity of nodes with their names:
from managing node try to connect via `ssh` to one of computing nodes, and from computing node try to connect to another node.
```
# as example
ssh kyiv1
# after connected to kyiv1
ssh kyiv2
```

### Adjust hostnames, so nodes understand they locating in cluster

From Node 0 we need to connect via `ssh` to each computational node and change the hostname.
based on names we agreed earlier:
```
10.42.42.10 kyiv1
10.42.42.11 kyiv2
10.42.42.12 kyiv3
10.42.42.13 kyiv4
```
As example:
```
#connect to Node 1
ssh scientist@10.42.42.10
# it will also ask for a password. Enter it
# now change the hostname to kyiv1
sudo hostnamectl set-hostname kyiv1
# now reboot the machine
sudo reboot
```
After that change the hostname on Node 0 and reboot the machine:
```
sudo hostnamectl set-hostname kyiv0
reboot
```
**NB !:** Reboot is not mandatory.

**NB! :** Semi-automatic way at this point would be running:
```
for i in kyiv{1..4}; do echo "$i"; ssh "$i" "sudo hostnamectl set-hostname $i"; done;
```
From the main node

### Verify connectivity with short names

Now, we can reach each node via short name:
```
ping -c 10 kyiv3
```
and it will return something like:
```
PING kyiv3 (10.42.42.12) 56(84) bytes of data.
64 bytes from kyiv3 (10.42.42.12): icmp_seq=1 ttl=64 time=0.859 ms
64 bytes from kyiv3 (10.42.42.12): icmp_seq=2 ttl=64 time=0.786 ms
64 bytes from kyiv3 (10.42.42.12): icmp_seq=3 ttl=64 time=0.723 ms
64 bytes from kyiv3 (10.42.42.12): icmp_seq=4 ttl=64 time=0.870 ms
64 bytes from kyiv3 (10.42.42.12): icmp_seq=5 ttl=64 time=0.918 ms
64 bytes from kyiv3 (10.42.42.12): icmp_seq=6 ttl=64 time=0.834 ms
64 bytes from kyiv3 (10.42.42.12): icmp_seq=7 ttl=64 time=0.759 ms
64 bytes from kyiv3 (10.42.42.12): icmp_seq=8 ttl=64 time=0.868 ms
64 bytes from kyiv3 (10.42.42.12): icmp_seq=9 ttl=64 time=0.692 ms
64 bytes from kyiv3 (10.42.42.12): icmp_seq=10 ttl=64 time=0.845 ms

--- kyiv3 ping statistics ---
10 packets transmitted, 10 received, 0% packet loss, time 179ms
rtt min/avg/max/mdev = 0.692/0.815/0.918/0.073 ms
```
Which means we did everything right.

# 2. Check if MUNGE is working
From main node run:
```
munge -n | ssh %IP_OF_ANY_COMPUTING_NODE% unmunge
```
let say: `munge -n | ssh 10.42.42.11 unmunge`
and it should return something like:
```
STATUS:           Success (0)
ENCODE_HOST:      localhost (127.0.0.1)
ENCODE_TIME:      2020-09-18 17:25:03 -0400 (1600464303)
DECODE_TIME:      2020-09-18 17:25:05 -0400 (1600464305)
TTL:              300
CIPHER:           aes128 (4)
MAC:              sha256 (5)
ZIP:              none (0)
UID:              swbec (1000)
GID:              sudo (1000)
LENGTH:           0
```

The same result must be returned if using nodename instead of its IP address: `munge -n | ssh kyiv1 unmunge`

# 3. SLURLM settings

### Collect configuration of each computation node
Connect to each node (let say, via `ssh`) and read the configuration of each machine can be read as `slurmd -C` on each node. The result should look like:
```
NodeName=kyiv1 CPUs=8 Boards=1 SocketsPerBoard=2 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=16028
UpTime=0-21:26:52
```
Copy this information without `UpTime` parameter. Edit the amount of available memory, to make it a little lower (for  system consumption).

So, after collection, you should have something like:
```
NodeName=kyiv1 CPUs=8 Boards=1 SocketsPerBoard=2 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=15028
NodeName=kyiv2 CPUs=8 Boards=1 SocketsPerBoard=2 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=15028
NodeName=kyiv3 CPUs=8 Boards=1 SocketsPerBoard=2 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=15028
NodeName=kyiv4 CPUs=8 Boards=1 SocketsPerBoard=2 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=15028
```

### Edit `slurmctld` confing on main node

Edit file `/etc/slurm/slurm.conf` on main node

Find lines with parameters `ControlMachine` and `ControlAddr`, and make them look as follow:
```
SlurmctldHost=kyiv0
SlurmctldAddr=10.42.42.1
```
Find line with parameter `ReturnToService` and change it as `ReturnToService=2`

Find lines with `SelectType` and `SelectTypeParameters` and replace those with:
```
SelectType=select/cons_res
SelectTypeParameters=CR_Core
```
Set the cluster name via editing the line with `ClusterName`:
```
ClusterName=kyiv_cluster
```

Now, go to the end of file, there should be something like:
```
# COMPUTE NODES
NodeName=localhost CPUs=1 State=UNKNOWN
PartitionName=debug Nodes=localhost Default=YES MaxTime=INFINITE State=UP
```
Change it to (according to previously collected information):
```
# COMPUTE NODES
NodeName=kyiv1 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=1435
NodeName=kyiv2 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=1435
NodeName=kyiv3 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=1435
NodeName=kyiv4 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=1435
# PARTITIONS
PartitionName=kyiv Nodes=kyiv[1-4] Default=YES MaxTime=INFINITE State=UP
```

**NB! :** It is also possible to make several partitions with the same node(s) in those, when one node can receive either one or multiple tasks simultaneously. If need to have several partitions, the end of file should look like:
```
# PARTITIONS
# Normal: One node receives one task only
PartitionName=kyiv Nodes=kyiv[1-4] Default=YES MaxTime=INFINITE State=UP Shared=EXCLUSIVE
# Shared: One node can accept more than one task, sharing resources is described in
# section `SCHEDULING`, parameters `SelectType` and `SelectTypeParameters`
PartitionName=kyiv-sh Nodes=kyiv[1-4] Default=NO MaxTime=INFINITE State=UP Shared=YES
```
It is highly not recommended to have the same node in both sharing and exclusive clusters, but it is possible.

**NB! :** Do not include the managing node to partition. Let it be a manager only! It is needed since it is ruling the DNS/DHCP/NFS and time servers.

Add ownership:
```
sudo chown slurm: /etc/slurm/slurm.conf
sudo touch /run/slurm/slurmctld.pid
sudo chown slurm: -R /run/slurm
sudo chown slurm: /run/slurm/slurmctld.pid
sudo mkdir /var/spool/slurmctld
sudo chown slurm: /var/spool/slurmctld
sudo chmod 755 /var/spool/slurmctld
sudo touch /var/log/slurmctld.log
sudo chown slurm: /var/log/slurmctld.log
sudo touch /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
sudo chown slurm: /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
```

```
sudo systemctl start slurmctld
sudo systemctl status slurmctld
slurmd -C
```

Copy config file to shared storage for easier later access from other nodes:
```
sudo cp /etc/slurm/slurm.conf /storage/slurm.conf
```


**NB! :** theoretically, may need to specify the IP addresses here too.
```
# COMPUTE NODES
NodeName=kyiv1 NodeAddr=10.42.42.10 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=600
NodeName=kyiv2 NodeAddr=10.42.42.11 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=600
NodeName=kyiv3 NodeAddr=10.42.42.12 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=600
PartitionName=kyiv Nodes=kyiv[1-3] Default=YES MaxTime=INFINITE State=UP
```
If you want that managing node was also a computing, that add it to this part too.


**NB !:** If something goes wrong, there is an [issue](https://medium.com/p/c901786cba54/responses/show) on Slurm 18.08 and Rasbpian Stretch: you need to find and modify the line with `ProctrackType` value.
Originally it is `ProctrackType=proctrack/cgroup`
```
ProctrackType=proctrack/linuxproc
```

### Copy the SLURM config to compute nodes

```
sudo cp /storage/shared/slurm.conf /etc/slurm/slurm.conf
sudo chown slurm: /etc/slurm/slurm.conf
sudo mkdir /var/spool/slurmd
sudo chown slurm: /var/spool/slurmd
sudo chmod 755 /var/spool/slurmd
sudo touch /var/log/slurmd.log
sudo chown slurm: /var/log/slurmd.log
```

```
sudo systemctl start slurmd
sudo systemctl status slurmd
slurmd -C
scontrol ping
```

**NB! :** If returns `Slurmctld(primary) at kyiv0 is DOWN` check if you didn't messed up with firewall settings!

**NB! :** Sometimes, it is needed to use:
```
sudo firewall-cmd --zone=internal --add-port={6817,6818,6819,7321}/tcp --permanent
sudo firewall-cmd --zone=internal --add-port={6817,6818,6819,7321}/udp --permanent
sudo firewall-cmd --reload
```

**NB! :** In rare cases, need to use:
```
sudo firewall-cmd --permanent --direct --add-rule ipv4 filter INPUT_direct 0 -s 10.42.42.0/24 -j ACCEPT
sudo firewall-cmd --reload
```


### Drained nodes
https://slurm-dev.schedmd.narkive.com/M4zI6M74/node-state-always-down-low-realmemory

Sometimes `sinfo -Nle` may show that node is drained.

If reason is `Low RealMemory` try from main node it means that software has lower memory that specified in `slurm.conf` file.
You can just run following from the main node
```
sudo scontrol update NodeName=kyiv[1-4] State=Resume
```
But wiser solution will be to lower value of real memory for problematic node in `slurm.conf` file.
The real value which can be seen by controller can be checked from file `/var/log/slurmctld.log`

Also, if there was a reboot of node (after upgrade of OS with further reboot), the node will be drained:
```
NODELIST   NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON
kyiv1          1      kyiv        down    4    1:4:1    600        0      1   (null) Node unexpectedly rebooted
```
It also can be fixed with the command, mentioned above or via editing `slurm.conf` and setting parameter `ReturnToService` to `2`


## Firewall security

### Blocking or dropping the outer connections
How to from [RHEL](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/security_guide/sec-using_firewalls#Configuring_Complex_Firewall_Rules_with_the_Rich-Language_Syntax)

1.
```
firewall-cmd --permanent --add-rich-rule="rule family='ipv4' source address='192.168.0.11' reject
```
where

2.
```
firewall-cmd --zone=drop --add-source=x.x.x.x/xx
```
replace `x.x.x.x` with the IP and you can add the subnet under `/xx`


Block following:
```
141.8.192.0/21
176.119.147.0/24
185.185.68.0/22
185.251.88.0/22
```
and :
```
141.8.196.176
141.8.196.56
141.8.192.31
```

---
[< PREVIOUS]() | [NEXT >]()
